{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f67fe5e-5529-4dfb-aa32-722dbc804de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Load data from Log Analytics to Delta Lake\n",
    "\n",
    "This notebook shows you how to import data from Log Analytics workspace into a Delta Lake table using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd15773-aafd-4b80-b2f5-08ee793d6996",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a79afd-7003-434a-84d2-873f5d8e48ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install azure-monitor-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad3bb4d-c27c-41ad-8dd8-0ee37c870a04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4edd802-9da0-40f0-a486-a7a21439dddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Variables\n",
    "db = \"deltadb\"\n",
    "table_format = \"delta\"\n",
    "table_name = \"process_execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a56edd-7bae-4c8e-9232-94368930a40e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Creates database if not exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "spark.sql(f\"USE {db}\")\n",
    "spark.sql(\"SET spark.databricks.delta.formatCheck.enabled = false\")\n",
    "spark.sql(\"SET spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cd7da0-0860-4b74-ae5f-82bc1527a03a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064e6786-c940-4ee4-a4e9-ae038155a1da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 1: Connection information\n",
    "\n",
    "First define some variables to programmatically create these connections.\n",
    "\n",
    "Replace all the variables in angle brackets `<>` below with the corresponding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86509b5-3bd9-412e-ab8f-b109dd1154d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Variables\n",
    "# The client_secret should be kept in the Databricks Secrets/Key Vault as security best practice.\n",
    "tenant_id = 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n",
    "workspace_id = 'www-xxx-yyy-zzz'\n",
    "client_id = 'yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy'\n",
    "# client_secret should be stored in Databricks Secrets/Key Vault\n",
    "# client_secret = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')\n",
    "client_secret = 'zzzzzz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb7ba9-2f26-42cc-910a-be69fa816bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Creates credential object and instantiate logs_client\n",
    "credential = ClientSecretCredential(\n",
    "       tenant_id=tenant_id,\n",
    "       client_id=client_id,\n",
    "       client_secret=client_secret,\n",
    "   )\n",
    "\n",
    "logs_client = LogsQueryClient(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165cb68e-c818-4c26-8157-2a47b553bb84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Reading the data\n",
    "\n",
    "Now that you've specified the file metadata, you can create a DataFrame. Use an *option* to infer the data schema from the file. You can also explicitly set this to a particular schema if you have one already.\n",
    "\n",
    "First, create a DataFrame in Python, referencing the variables defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b34fa8-1e55-41aa-926f-60e17096434c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Query parameters\n",
    "query = \"\"\"SecurityEvent\n",
    "| where EventID == 4688\n",
    "| where Account != @'-\\-'\n",
    "| project TimeGenerated, Account, AccountType, Computer, Process, CommandLine\"\"\"\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(days = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba19b8e-925e-461a-9c38-ae7d5f4fa40a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Retrieve data\n",
    "try:\n",
    "    response = logs_client.query_workspace(\n",
    "        workspace_id=workspace_id,\n",
    "        query=query,\n",
    "        timespan=(start_time, end_time)\n",
    "        )\n",
    "    if response.status == LogsQueryStatus.PARTIAL:\n",
    "        error = response.partial_error\n",
    "        data = response.partial_data\n",
    "        print(error)\n",
    "    elif response.status == LogsQueryStatus.SUCCESS:\n",
    "        data = response.tables\n",
    "    for table in data:\n",
    "        # Converting the table data to a Spark DataFrame\n",
    "        pandas_df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
    "        df=spark.createDataFrame(pandas_df) \n",
    "except HttpResponseError as err:\n",
    "    print(\"something fatal happened\")\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2818b1-ae98-45a4-9ad4-a5afefa31468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DF Schema\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd062767-206e-4be9-b418-818f802a2287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3: Create a Delta table\n",
    "\n",
    "The DataFrame defined and displayed above is a temporary connection to the remote database.\n",
    "\n",
    "To ensure that this data can be accessed by relevant users throughout your workspace, save it as a Delta Lake table using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44724df6-f755-456f-9b74-ac16cc94be94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5cbc2d-63f8-49a6-bc52-65e965190385",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This table will persist across cluster sessions, notebooks, and personas throughout your organization.\n",
    "\n",
    "The code below demonstrates querying this data with Python and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb76bb4f-ed3f-4e59-9957-17b14ff4e281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display data table\n",
    "display(spark.table(table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 702684524612725,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_PoC_AnomalousPE_DataIngestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

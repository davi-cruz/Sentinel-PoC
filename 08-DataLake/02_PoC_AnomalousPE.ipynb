{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b45c2fb1-8e9a-4fc0-9794-cf2ef9b3e4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Anomalous Process Execution\n",
    "\n",
    "Read data from the previously created table and creates a model to determine anomalous process executions within 4688 Event Logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845cbb11-fcb6-4965-8859-c2b0efd3f3aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5963749-5c48-4bf5-a9d9-b6d50fcca1f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Variables\n",
    "\n",
    "# Delta table info\n",
    "db = \"deltadb\"\n",
    "table_format = \"delta\"\n",
    "table_name = \"process_execution\"\n",
    "\n",
    "# Log Analytics workspace info\n",
    "workspace_id = 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n",
    "# workspace_shared_key should be stored in Databricks Secrets/Key Vault\n",
    "# workspace_shared_key = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')\n",
    "workspace_shared_key = 'yyyyyyyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3ad688-a457-4838-a240-ab8a0942b9c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set context accordingly and list tables\n",
    "spark.sql(f\"USE {db};\")\n",
    "spark.sql(\"SHOW TABLES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b197c8-a590-43a1-be60-18563e301e84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a52d2a-2d21-4185-8cee-f2e32cfd9217",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load data and summarize by hour\n",
    "data = spark.read.format(table_format).table(table_name)\n",
    "#data = data.withColumn(\"interval\", date_trunc(\"hour\", \"TimeGenerated\"))\n",
    "data = data.withColumn(\"interval\", date_trunc(\"day\", \"TimeGenerated\"))\n",
    "data = data.groupBy(\"interval\", \"Computer\", \"Account\", \"AccountType\", \"Process\").agg(collect_set(\"CommandLine\").alias(\"CommandLines\"), count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be49cdd-3d47-4af4-b086-9b87cfa0dba2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Prepare data for vectorizing\n",
    "\n",
    "# Convert hour column to Unix timestamp\n",
    "data = data.withColumn(\"unixInterval\", unix_timestamp(\"interval\"))\n",
    "\n",
    "# Convert categorical columns to numerical values using StringIndexer\n",
    "computerIndexer = StringIndexer(inputCol=\"Computer\", outputCol=\"computerIndex\")\n",
    "accountIndexer = StringIndexer(inputCol=\"Account\", outputCol=\"accountIndex\")\n",
    "accountTypeIndexer = StringIndexer(inputCol=\"AccountType\", outputCol=\"accountTypeIndex\")\n",
    "processIndexer = StringIndexer(inputCol=\"Process\", outputCol=\"processIndex\")\n",
    "data = computerIndexer.fit(data).transform(data)\n",
    "data = accountIndexer.fit(data).transform(data)\n",
    "data = accountTypeIndexer.fit(data).transform(data)\n",
    "data = processIndexer.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f998e6d4-3d62-4b45-a252-c792bdb64a84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9548ab06-bbb6-4efe-8ed6-635df16a63f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Creates Isolation Forest model and fit it\n",
    "\n",
    "features = [\"unixInterval\", \"computerIndex\", \"accountIndex\", \"accountTypeIndex\", \"processIndex\",\"count\"]\n",
    "clf = IsolationForest(max_samples=100, random_state=42)\n",
    "clf.fit(np.array(data.select(features).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a373581-8499-4c6f-aa30-cb4b9ce75d6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create predictions based on trained model\n",
    "y_pred = (clf.predict(data.select(features).collect())).tolist()\n",
    "indexed_y_pred =  [(i, n) for i, n in enumerate(y_pred)]\n",
    "y_pred_df = spark.createDataFrame(indexed_y_pred, [\"index\",\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29da06f7-eaaa-436e-b1d5-6ddeda8057f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Append score to dataframe and filter anomalies\n",
    "data = data.withColumn(\"index\", monotonically_increasing_id())\n",
    "data = data.join(y_pred_df, data.index == y_pred_df.index).drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd271820-cb03-4350-b108-27aca99432c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Filter Anomalies \n",
    "\n",
    "now = datetime.now(timezone.utc)\n",
    "timeInterval = now - timedelta(days = 1)\n",
    "anomalies = data.filter((col(\"score\") == -1) & (col(\"interval\") >= timeInterval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6171a3a-463a-49f7-84eb-64a6c0837c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display anomalies\n",
    "display(anomalies.select([\"interval\", \"Computer\", \"Account\", \"AccountType\", \"Process\",\"CommandLines\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d36bdad9-a3ad-420f-8341-f8ef22fbd609",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Send Anomalies to Log Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4259c3ff-ed11-435b-8d2b-27195dd88a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @udf\n",
    "# def escape_str(str):\n",
    "#   return str.replace('\\\\','\\\\\\\\')\n",
    "\n",
    "# def send_results_to_log_analytics(df_to_la):\n",
    "#   # The log type is the name of the event that is being submitted.  This will show up under \"Custom Logs\" as log_type + '_CL'\n",
    "#   log_type = 'AnomalousProcessExecutionResult'\n",
    "\n",
    "#   # concatenate columns to form one json record\n",
    "#   json_records = df_to_la.withColumn('json_field', f.concat(f.lit('{'), \n",
    "#                                             f.lit(' \\\"TimeStamp\\\": \\\"'), f.from_unixtime(f.unix_timestamp(f.col(\"timestamp\")), \"y-MM-dd'T'hh:mm:ss.SSS'Z'\"), f.lit('\\\",'),\n",
    "#                                             f.lit(' \\\"User\\\": \\\"'), escape_str(f.col('user')), f.lit('\\\",'),\n",
    "#                                             f.lit(' \\\"Resource\\\": \\\"'), escape_str(f.col('res')), f.lit('\\\",'),\n",
    "#                                             f.lit(' \\\"AnomalyScore\\\":'), f.col('anomaly_score'),\n",
    "#                                             f.lit('}')\n",
    "#                                            )                       \n",
    "#                                          )\n",
    "#   # combine json record column to create the array\n",
    "#   json_body = json_records.agg(f.concat_ws(\", \", f.collect_list('json_field')).alias('body'))\n",
    "\n",
    "#   if len(json_body.first()) > 0:\n",
    "#     json_payload = json_body.first()['body']\n",
    "#     json_payload = '[' + json_payload + ']'\n",
    "\n",
    "#     payload = json_payload.encode('utf-8') #json.dumps(json_payload)\n",
    "#     # print(payload)\n",
    "#     return log_analytics_client(workspace_id, workspace_shared_key).post_data(payload, log_type)\n",
    "#   else:\n",
    "#     return \"No json data to send to LA\"\n",
    "\n",
    "# count = results_to_la.count()\n",
    "# if count > 0:\n",
    "#   print ('Results count = ', count)\n",
    "#   result = send_results_to_log_analytics(results_to_la)\n",
    "#   print(\"Writing to Log Analytics result: \", result)\n",
    "# else:\n",
    "#   print ('No results to send to LA')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_PoC_AnomalousPE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
